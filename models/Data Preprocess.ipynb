{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c08adc6-17a5-47a7-9e4c-8b7df5284b59",
   "metadata": {},
   "source": [
    "# PGS data processing\n",
    "\n",
    "This script is specifically designed to automate the process of downloading Alzheimer's Disease (AD) research data from the PGS catalog. It focuses on extracting files from FTP links provided in a CSV file and ensures that downloaded data undergoes preliminary data cleaning and quality control.\n",
    "\n",
    "Script Purpose\n",
    "*  Targeted Downloading: Automates the downloading of AD research data files from the PGS catalog, addressing the needs of researchers focusing on genetic contributions to Alzheimer's Disease.\n",
    "* Data Cleaning and Quality Control: While the script prepares data for cleaning and quality checks, these processes are conducted separately to ensure data integrity and usability.\n",
    "s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc54043-4383-4c0a-9ff7-63ad59065942",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918decf-e57f-401f-b323-727e75609002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0ae7e-8f33-4b8f-92ea-1d6331d69740",
   "metadata": {},
   "source": [
    "## Data Download And Decompress\n",
    "1. **Reads the CSV**: Loads the CSV file and extracts file URLs from the specified column.\n",
    "2. **Downloads Files**: Downloads each file from its URL to the specified folder.\n",
    "3. **Checks Existing Files**: Prevents re-downloading files that already exist.\n",
    "4. **Logs Status**: Outputs the status of each file download, noting both successes and failures.\n",
    "5. **Decompress**: Outputs the decompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b1752-2f9f-4aa4-8302-fff1792a9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the CSV file and the download directory\n",
    "csv_file_path = os.path.join(r\"C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\", \"pgs_scores_Alzheimer.csv\")\n",
    "download_folder = os.path.join(r\"C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\", \"AlzheimerPGS\")\n",
    "\n",
    "# Ensure the download directory exists\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "# Specify the directory where decompressed files should be stored\n",
    "decompression_folder = os.path.join(download_folder, \"PGS_Decompress\")\n",
    "\n",
    "# Ensure the decompression directory exists\n",
    "os.makedirs(decompression_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba73121-c52d-479a-aa33-03238a4d28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the CSV data\n",
    "df = pd.read_csv(csv_file_path)\n",
    "compressed_files = []\n",
    "# Process each row to handle file downloads and renaming\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Downloading and decompressing files\"):\n",
    "    file_url = row['Scoring File (FTP Link)']  # Extract the FTP link\n",
    "    original_filename = file_url.split('/')[-1]  # Get the original filename from the URL\n",
    "    \n",
    "    core_filename = original_filename.split('.')[0]  # Extract the core identifier before the first '.'\n",
    "\n",
    "    file_url = file_url.replace(original_filename, 'Harmonized/'+core_filename +'_hmPOS_GRCh38.txt.gz')\n",
    "    file_path = os.path.join(download_folder, core_filename +'_hmPOS_GRCh38.txt.gz')  # Update the path with the new filename\n",
    "    compressed_files.append(core_filename +'_hmPOS_GRCh38.txt.gz')\n",
    "    # Check if the new file already exists\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            print(f\"Downloading and renaming {original_filename} to {core_filename}...\")\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            else:\n",
    "                print(f\"Failed to download {original_filename}. Status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while downloading {original_filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"{new_filename} already exists.\")\n",
    "print(f'Download complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7191d-2a27-4595-a94c-f93813982967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all .gz files in the download directory\n",
    "gz_files = [f for f in os.listdir(download_folder) if f.endswith('.gz')]\n",
    "\n",
    "# Iterate over the list of files and decompress each one\n",
    "for gz_file in tqdm(gz_files, desc='Decompressing files'):\n",
    "    file_path = os.path.join(download_folder, gz_file)\n",
    "    decompressed_file_path = os.path.join(decompression_folder, gz_file[:-3])  # Removes '.gz'\n",
    "\n",
    "    # Open the gzipped file and decompress it\n",
    "    with gzip.open(file_path, 'rb') as gzipped_file:\n",
    "        with open(decompressed_file_path, 'wb') as decompressed_file:\n",
    "            decompressed_file.write(gzipped_file.read())\n",
    "            print(f\"Decompressed {gz_file} to {decompressed_file_path}\")\n",
    "\n",
    "print(\"All files have been decompressed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66b1a2-5dc1-4fc3-a778-ea29505797ef",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdd09d-da97-49d9-93d5-9b9155ba35c2",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "1. **Annotation**: we run the pipeline to annotate the files and fill the missing data with NA\n",
    "2. **Removing the unknown rsID**: remove the unknown rsID\n",
    "3. **File selection**: exclude the file raise errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ce893-c74e-4c2a-ad01-32a21fdee8cf",
   "metadata": {},
   "source": [
    "### For Files with issues, we annoate the hm_rsID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052121b-f703-4c66-ad53-b7e9969484d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_filenames = ['PGS003957','PGS003958']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b576d2-6c74-442f-b57f-9a2ee2bce554",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "1. **scan all files and get a annotation map**: the annotation map has columns of ['SNP_coord', 'hm_rsID', 'hm_chr', 'hm_pos', 'effect_allele'] with no duplicate variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe0189-3b63-4153-920d-27cd20730e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing your CSV files\n",
    "directory = r'C:\\Users\\gqu\\Desktop\\projects\\Genevic\\data\\AlzheimerPGS\\Annotated'\n",
    "\n",
    "# List to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all the CSV files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)  # Read the CSV file\n",
    "        dataframes.append(df)  # Append the DataFrame to the list\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "full_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Select unique rows based on specific columns\n",
    "annotation_map = full_df[['SNP_coord', 'hm_rsID', 'hm_chr', 'hm_pos', 'effect_allele']].drop_duplicates()\n",
    "\n",
    "# Optionally, save the unique DataFrame to a new CSV file\n",
    "annotation_map.to_csv(r'C:\\Users\\gqu\\Desktop\\projects\\Genevic\\data\\annotation_map.csv', index=False)\n",
    "\n",
    "# Print the unique DataFrame\n",
    "print(annotation_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007092d-8807-43fe-9bcd-b537ac2a8ab0",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "1. **Detect duplicates**: Duplicate CSV files are defined as the files containing the same set of rsID\n",
    "2. **Merge duplicates**: Convert the effect weight to z-score and merge the duplicates by sum, append the ranks into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c06d9d-1fdd-49c0-9f04-8f8d3d637952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        rootX = self.find(x)\n",
    "        rootY = self.find(y)\n",
    "        if rootX != rootY:\n",
    "            self.parent[rootY] = rootX\n",
    "def extract_file_id(filename):\n",
    "    # Define a helper to extract and validate the file ID from a filename\n",
    "    match = re.match(r\"(PGS\\d+)_\", filename)\n",
    "    return match.group(1) if match else None\n",
    "def find_duplicate_files(folder_path):\n",
    "    file_data = {}\n",
    "    file_sets = {}\n",
    "    uf = UnionFind()\n",
    "    \n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_id = extract_file_id(filename)\n",
    "            if file_id:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Initialize union-find\n",
    "                uf.parent[file_id] = file_id\n",
    "                \n",
    "                # Check for necessary columns\n",
    "                if 'SNP_coord' in df.columns and 'hm_rsID' in df.columns:\n",
    "                    # Create sets of 'SNP_coord' and 'hm_rsID'\n",
    "                    snp_set = frozenset(df['SNP_coord'])\n",
    "                    rsid_set = frozenset(df['hm_rsID'])\n",
    "                    \n",
    "                    # Store in dictionaries for comparison\n",
    "                    file_data[file_id] = df\n",
    "                    file_sets[file_id] = {'SNP_coord': snp_set, 'hm_rsID': rsid_set}\n",
    "\n",
    "    # Identify matching files\n",
    "    file_ids = list(file_sets.keys())\n",
    "    \n",
    "    for i in range(len(file_ids)):\n",
    "        for j in range(i + 1, len(file_ids)):\n",
    "            id1, id2 = file_ids[i], file_ids[j]\n",
    "            if file_sets[id1]['SNP_coord'] == file_sets[id2]['SNP_coord'] or file_sets[id1]['hm_rsID'] == file_sets[id2]['hm_rsID']:\n",
    "                uf.union(id1, id2)\n",
    "\n",
    "    # Consolidate matches into groups, ensuring no self-only groups unless necessary\n",
    "    groups = {}\n",
    "    for file_id in file_ids:\n",
    "        root = uf.find(file_id)\n",
    "        groups.setdefault(root, []).append(file_id)\n",
    "    \n",
    "    # Filter out groups where each ID is only self-referencing, unless no other IDs are in the group\n",
    "    final_groups = {k: v for k, v in groups.items() if len(v) > 1 or len(set(v)) > 1}\n",
    "\n",
    "    return final_groups, file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787cfe4-72b3-48eb-8ba0-91e1e5ddc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,List, Dict\n",
    "\n",
    "def convert_to_z_scores(\n",
    "    file_path: str,\n",
    "    effect_size_col: str = 'effect_weight',\n",
    "    rank_col: str = 'ranks',\n",
    "    effect_allele_col: str = 'effect_allele',\n",
    "    multiply_by_effect_allele_count: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file, optionally multiplies the absolute value of effect_weight by the number of effect alleles\n",
    "    computed from effect_allele, converts the effect weights to Z-scores, and retains hm_rsID,\n",
    "    effect_weight (Z-score), and ranks.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the CSV file.\n",
    "    - effect_size_col: Name of the column containing effect weights.\n",
    "    - rank_col: Name of the column containing ranks.\n",
    "    - effect_allele_col: Name of the column containing effect alleles.\n",
    "    - multiply_by_effect_allele_count: If True, multiplies the absolute value of effect_weight by the number of effect alleles\n",
    "      before calculating Z-scores.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns: hm_rsID, effect_weight (Z-score), ranks.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_columns = ['hm_rsID', effect_size_col, rank_col, effect_allele_col]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in the file '{file_path}'.\")\n",
    "\n",
    "    # Optionally multiply the absolute value of effect_weight by the number of effect alleles\n",
    "    if multiply_by_effect_allele_count:\n",
    "        # Define a function to compute effect_allele_count\n",
    "        def compute_effect_allele_count(effect_allele):\n",
    "            if pd.isnull(effect_allele):\n",
    "                return 0\n",
    "            else:\n",
    "                # Remove non-letter characters and count letters\n",
    "                letters = re.findall(r'[A-Za-z]', effect_allele)\n",
    "                return len(letters)\n",
    "        \n",
    "        # Compute effect_allele_count for each row\n",
    "        df['effect_allele_count'] = df[effect_allele_col].apply(compute_effect_allele_count)\n",
    "        # Multiply the absolute value of effect_weight by effect_allele_count\n",
    "        df[effect_size_col] = np.abs(df[effect_size_col]) * df['effect_allele_count']\n",
    "    else:\n",
    "        # Convert effect_weight to its absolute value\n",
    "        df[effect_size_col] = np.abs(df[effect_size_col])\n",
    "\n",
    "    # Calculate mean and standard deviation of effect weights\n",
    "    mu = df[effect_size_col].mean()\n",
    "    sigma = df[effect_size_col].std()\n",
    "\n",
    "    # Handle the case where standard deviation is zero\n",
    "    if sigma == 0 or pd.isnull(sigma):\n",
    "        df['effect_weight_z'] = 0\n",
    "    else:\n",
    "        # Compute Z-scores using the absolute values\n",
    "        df['effect_weight_z'] = (df[effect_size_col] - mu) / sigma\n",
    "\n",
    "    # Select the required columns\n",
    "    df_z = df[['hm_rsID', 'effect_weight_z', rank_col]].copy()\n",
    "    df_z.rename(columns={'effect_weight_z': 'effect_weight'}, inplace=True)\n",
    "\n",
    "    return df_z\n",
    "\n",
    "\n",
    "def merge_duplicate_dataframes(data_dict, duplicate_file_groups, effect_size_col='effect_weight', rank_col='ranks'):\n",
    "    \"\"\"\n",
    "    Merges DataFrames in data_dict that correspond to duplicate files, summing effect_weight and collecting ranks into a list.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict: Dictionary with file_id as keys and DataFrames as values.\n",
    "    - duplicate_file_groups: Dictionary where each key is a group representative file_id,\n",
    "                             and the value is a list of duplicate file_ids (including the key).\n",
    "    - effect_size_col: Name of the effect weight column in the DataFrames.\n",
    "    - rank_col: Name of the rank column in the DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - A new data_dict with merged DataFrames for duplicate files.\n",
    "    \"\"\"\n",
    "    # Create a copy of data_dict to avoid modifying the original\n",
    "    merged_data_dict = data_dict.copy()\n",
    "    \n",
    "    # Keep track of file_ids that have been merged and should be removed\n",
    "    file_ids_to_remove = set()\n",
    "    \n",
    "    for group_id, file_ids in duplicate_file_groups.items():\n",
    "        # List to store DataFrames to be merged\n",
    "        dfs_to_merge = []\n",
    "        \n",
    "        for file_id in file_ids:\n",
    "            if file_id in merged_data_dict:\n",
    "                dfs_to_merge.append(merged_data_dict[file_id])\n",
    "                # Mark the file_id for removal if it's not the group_id\n",
    "                if file_id != group_id:\n",
    "                    file_ids_to_remove.add(file_id)\n",
    "            else:\n",
    "                print(f\"Warning: file_id '{file_id}' not found in data_dict.\")\n",
    "        \n",
    "        if dfs_to_merge:\n",
    "            # Concatenate the DataFrames\n",
    "            combined_df = pd.concat(dfs_to_merge, ignore_index=True)\n",
    "            # Group by 'hm_rsID' and aggregate\n",
    "            merged_df = combined_df.groupby('hm_rsID').agg({\n",
    "                effect_size_col: 'sum',\n",
    "                rank_col: lambda x: list(x)\n",
    "            }).reset_index()\n",
    "            # Update the data_dict with the merged DataFrame\n",
    "            merged_data_dict[group_id] = merged_df\n",
    "        else:\n",
    "            print(f\"No DataFrames found for group '{group_id}'.\")\n",
    "    \n",
    "    # Remove the merged file_ids from data_dict\n",
    "    for file_id in file_ids_to_remove:\n",
    "        del merged_data_dict[file_id]\n",
    "    \n",
    "    return merged_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30c9e2-3711-4876-a86f-66ed0583e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_N(df, N=200, weight_column='effect_weight'):\n",
    "    if len(df) > N:\n",
    "        # Sort by 'effect_weight' in descending order and keep the top N\n",
    "        return df.nlargest(N, weight_column)\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ff5a5-623e-4162-a643-6953516df3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "\n",
    "def filter_top_N_advanced(df, N=200, weight_column='effect_weight'):\n",
    "    \"\"\"\n",
    "    Advanced Top-N Filtering:\n",
    "    1. If df has more than N rows, determine a quantile cutoff for top fraction (N/len(df)).\n",
    "    2. Filter rows above this threshold.\n",
    "    3. If still more than N remain, select the top N.\n",
    "    Otherwise, return df as-is if ≤ N.\n",
    "    \"\"\"\n",
    "    if len(df) > N:\n",
    "        quantile_cutoff = 1 - (N / len(df))\n",
    "        threshold = df[weight_column].quantile(quantile_cutoff)\n",
    "        filtered = df[df[weight_column] >= threshold]\n",
    "\n",
    "        if len(filtered) > N:\n",
    "            return filtered.nlargest(N, weight_column)\n",
    "        else:\n",
    "            return filtered\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def overlap_metric(variant_sets):\n",
    "    \"\"\"\n",
    "    Compute a partial overlap metric based on how many studies each variant appears in.\n",
    "    Uses sum over variants of C(count, 2).\n",
    "    \"\"\"\n",
    "    all_variants = [v for s in variant_sets for v in s]\n",
    "    counts = Counter(all_variants)\n",
    "    score = sum((c*(c-1))//2 for c in counts.values())\n",
    "    return score\n",
    "\n",
    "def maximize_overlap_fraction_threshold(study_dataframes, id_column='hm_rsID', \n",
    "                                        weight_column='effect_weight', max_N=500, fraction=0.9):\n",
    "    \"\"\"\n",
    "    Choose N as the smallest N that achieves at least 'fraction' of the maximum overlap score.\n",
    "\n",
    "    Parameters:\n",
    "    - study_dataframes: list of pd.DataFrame, one per study.\n",
    "    - id_column: name of the variant ID column.\n",
    "    - weight_column: name of the effect weight column.\n",
    "    - max_N: maximum N to evaluate.\n",
    "    - fraction: the fraction of the maximum overlap score we want to achieve.\n",
    "\n",
    "    Returns:\n",
    "    - chosen_N: the smallest N that achieves at least 'fraction' * max overlap.\n",
    "    - chosen_score: the overlap score at chosen_N.\n",
    "    - max_score: the maximum overlap score achieved at any N.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for N in range(1, max_N + 1):\n",
    "        filtered_dataframes = [filter_top_N_advanced(df, N=N, weight_column=weight_column) for df in study_dataframes]\n",
    "        variant_sets = [set(df[id_column]) for df in filtered_dataframes if not df.empty and id_column in df.columns]\n",
    "        \n",
    "        if variant_sets:\n",
    "            current_score = overlap_metric(variant_sets)\n",
    "        else:\n",
    "            current_score = 0\n",
    "        scores.append((N, current_score))\n",
    "    \n",
    "    # Find the maximum score\n",
    "    max_score = max(score for _, score in scores)\n",
    "    \n",
    "    # Compute the threshold score\n",
    "    threshold_score = fraction * max_score\n",
    "    \n",
    "    # Find the smallest N that meets or exceeds the threshold score\n",
    "    # If all scores are zero, it will pick N=1.\n",
    "    chosen_N, chosen_score = min((N_s for N_s in scores if N_s[1] >= threshold_score),\n",
    "                                 key=lambda x: x[0],\n",
    "                                 default=(1, 0))  # default if none meets threshold\n",
    "    \n",
    "    return chosen_N, chosen_score, max_score\n",
    "\n",
    "# Example usage:\n",
    "# chosen_N, chosen_score, max_score = maximize_overlap_fraction_threshold(study_dataframes_list, fraction=0.9)\n",
    "# print(f\"Chosen N: {chosen_N}, Overlap Score at N: {chosen_score}, Max Score: {max_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153de83c-97a9-4075-b791-dd13476e337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from collections import Counter\n",
    "study_frames = []\n",
    "for file_id, df in merged_data_dict.items():\n",
    "    df_copy = df.copy(deep=True)\n",
    "    study_frames.append(df_copy)\n",
    "chosen_N, chosen_score, max_score = maximize_overlap_fraction_threshold(study_dataframes=study_frames, id_column='hm_rsID', \n",
    "                                        weight_column='effect_weight', max_N=500, fraction=0.9)\n",
    "\n",
    "print(chosen_N, chosen_score, max_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941079eb-07e4-4f0f-a524-2dee9b035588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_effect_weights(merged_data_dict, effect_size_col='effect_weight', rank_col='ranks'):\n",
    "    \"\"\"\n",
    "    Standardizes the summed effect_weight for each hm_rsID in the merged_data_dict and replaces the list of ranks \n",
    "    with their average. Then, reranks the hm_rsIDs based on these averages.\n",
    "\n",
    "    Parameters:\n",
    "    - merged_data_dict: Dictionary where each key is a file_id and the value is a merged DataFrame.\n",
    "    - effect_size_col: Name of the effect weight column (default is 'effect_weight').\n",
    "    - rank_col: Name of the rank column containing lists of ranks (default is 'ranks').\n",
    "\n",
    "    Returns:\n",
    "    - A new dictionary with standardized effect weights and updated ranks for each DataFrame.\n",
    "    \"\"\"\n",
    "    standardized_data_dict = {}\n",
    "\n",
    "    for file_id, df in merged_data_dict.items():\n",
    "        df_copy = df.copy(deep=True)\n",
    "        df_copy = filter_top_N(df_copy, N=chosen_N)\n",
    "        # Calculate the average rank where ranks are in a list and update the ranks column\n",
    "        df_copy[rank_col] = df_copy[rank_col].apply(lambda x: np.mean(x) if isinstance(x, list) else x)\n",
    "\n",
    "        # Calculate the standard deviation based on the count of elements used to compute each average rank\n",
    "        df_copy['std_dev'] = df_copy[rank_col].apply(lambda x: np.sqrt(len(x)) if isinstance(x, list) else 1)\n",
    "\n",
    "        # Standardize the effect_weight\n",
    "        df_copy[effect_size_col] = df_copy.apply(\n",
    "            lambda row: row[effect_size_col] / row['std_dev'] if row['std_dev'] > 0 else np.nan,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Drop the intermediate column\n",
    "        df_copy.drop(columns=['std_dev'], inplace=True)\n",
    "\n",
    "        # Rerank based on the updated single rank values\n",
    "        df_copy.sort_values(by=rank_col, ascending=True, inplace=True)\n",
    "        df_copy[rank_col] = range(1, len(df_copy) + 1)\n",
    "\n",
    "        # Update the DataFrame in the new dictionary\n",
    "        standardized_data_dict[file_id] = df_copy\n",
    "\n",
    "    return standardized_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa31502-2bcd-4f5e-a1c9-bac426d6a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "folder_path = r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\AlzheimerPGS\\processed'\n",
    "matches, file_data = find_duplicate_files(folder_path)\n",
    "print(\"Duplicate File Matches:\", matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adcdc6d-7856-40ff-8426-28811c49bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for a single file\n",
    "file_path = r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\AlzheimerPGS\\Annotated\\PGS000026_annotated_dataset.csv'\n",
    "df_z = convert_to_z_scores(file_path)\n",
    "\n",
    "# View the result\n",
    "print(df_z)\n",
    "print(df_z['effect_weight'].sum())\n",
    "file_path = r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\AlzheimerPGS\\Annotated\\PGS000876_annotated_dataset.csv'\n",
    "df_z = convert_to_z_scores(file_path)\n",
    "\n",
    "# View the result\n",
    "print(df_z)\n",
    "print(df_z['effect_weight'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec73eb-ce42-4ae5-95c9-1c950e512e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_id = extract_file_id(filename)\n",
    "        if file_id:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df_z = convert_to_z_scores(file_path, multiply_by_effect_allele_count=True)\n",
    "            dataset[file_id] = df_z\n",
    "merged_data_dict = merge_duplicate_dataframes(dataset, matches)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4697edb-d680-4e5b-a2b1-cc9db1788100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(merged_data_dict.keys())\n",
    "# print(len(merged_data_dict.keys()))\n",
    "# print(merged_data_dict['PGS000026'])\n",
    "# print(merged_data_dict['PGS000026']['effect_weight'].sum())\n",
    "# print(merged_data_dict['PGS000811'])\n",
    "# print(merged_data_dict['PGS000811']['effect_weight'].sum())\n",
    "# print(merged_data_dict['PGS000025'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d74f7e-89a0-49ff-a8c7-c33d75fc9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = standardize_effect_weights(merged_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92681c7-0e7e-463a-b6c7-0a00eac7affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\data_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead2490-ca12-40c3-9d42-f89a7e824d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(merged_data_dict['PGS000026'])\n",
    "# print(merged_data_dict['PGS000026']['effect_weight'].sum())\n",
    "# print(merged_data_dict['PGS000811'])\n",
    "# print(merged_data_dict['PGS000811']['effect_weight'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dafb8-946e-4842-b18c-260311af5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data_dict['PGS000026'])\n",
    "print(processed_data['PGS000026'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754a5fb-9019-47d8-97be-54cc03caf83d",
   "metadata": {},
   "source": [
    "### We have the processed data \n",
    "1. **processed_data** saved as \"data_dict.pkl\"\n",
    "2. **annotation_map** saved as \"annotation_map.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238393c4-5765-45fa-9e24-5917471f0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we can import those data\n",
    "# Load annotation map\n",
    "annotation_map_check = pd.read_csv(r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\annotation_map.csv', low_memory=False)\n",
    "# Load the dictionary back from the pickle file\n",
    "with open(r'C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\\data_dict.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "print(annotation_map_check.head)\n",
    "print(data_dict.keys())\n",
    "print(data_dict['PGS000026'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb7dad-f32e-4585-b531-d55aadc8bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PGS003992\n",
    "print(data_dict['PGS003992'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da5be1-46d8-41e5-a3b1-b20b2f4ad08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "labels_df = pd.read_csv(os.path.join(r\"C:\\Users\\gqu\\OneDrive - UTHealth Houston\\projects\\Genevic\\data\",\"AD_GWAS_Priority_Scores.csv\"))\n",
    "filtered_df = labels_df[labels_df['priority_score'].notna()]\n",
    "\n",
    "# Display the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(filtered_df['priority_score'], bins=20, edgecolor='black')\n",
    "plt.title('Distribution of Priority Score')\n",
    "plt.xlabel('Priority Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.savefig('y_dist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2ba84-a64c-4047-b4e8-4d55e3a50f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
